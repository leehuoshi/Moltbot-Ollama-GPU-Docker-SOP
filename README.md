# Moltbot-Ollama-GPU-Docker-SOP
åœ¨ Windows ä¸»æ©Ÿä¸Šï¼Œä½¿ç”¨ Docker Compose å•Ÿå‹• Moltbot Gateway èˆ‡ CLIï¼Œ ä¸¦æˆåŠŸé€éŽ æœ¬åœ° Ollamaï¼ˆGPUï¼‰æ¨¡åž‹åŸ·è¡Œ Moltbot Agentã€‚

0ï¸âƒ£ ç’°å¢ƒå‰ç½®æ¢ä»¶
ä¸»æ©Ÿç’°å¢ƒ
Windows 10 / 11
NVIDIA GPUï¼ˆå·²å®‰è£é©…å‹•ï¼‰
Docker Desktopï¼ˆå•Ÿç”¨ WSL2ï¼‰
Ollamaï¼ˆWindows ç‰ˆï¼‰

1.ç¢ºèª Ollama æ­£å¸¸ï¼ˆåœ¨ä¸»æ©Ÿ PowerShellï¼‰
ollama list
ollama run qwen2.5:7b-instruct-q4_K_M "hello"


ä¸¦ç¢ºèª API å¯å­˜å–ï¼š
curl http://localhost:11434/api/tags

1ï¸âƒ£ å»ºç«‹å°ˆæ¡ˆçµæ§‹
Moltbot/
```text
Moltbot/
â”œâ”€ docker-compose.yml
â”œâ”€ Dockerfile
â””â”€ README.md
```

## 2ï¸âƒ£ docker-compose.yml

```yaml
services:
  moltbot-gateway:
    image: moltbot:local
    build: .
    environment:
      HOME: /home/node
      TERM: xterm-256color
    volumes:
      - clawdbot-config:/home/node/.clawdbot
      - clawdbot-workspace:/home/node/clawd
    ports:
      - "18789:18789"
      - "18790:18790"
    restart: unless-stopped
    command:
      [
        "node",
        "dist/index.js",
        "gateway",
        "--allow-unconfigured",
        "--bind",
        "lan",
        "--port",
        "18789"
      ]

  moltbot-cli:
    image: moltbot:local
    build: .
    environment:
      HOME: /home/node
      TERM: xterm-256color
    volumes:
      - clawdbot-config:/home/node/.clawdbot
      - clawdbot-workspace:/home/node/clawd
    stdin_open: true
    tty: true
    entrypoint: ["node", "dist/index.js"]

volumes:
  clawdbot-config:
  clawdbot-workspace:
```


3ï¸âƒ£ å•Ÿå‹• Moltbot Gateway
docker compose up -d


ç¢ºèªå®¹å™¨ç‹€æ…‹ï¼š

docker ps

4ï¸âƒ£ è¨­å®š Moltbot ä½¿ç”¨ Ollama

Ollama ä¸èµ° agent auth-profiles

å¿…é ˆè¨­å®šåœ¨ ~/.clawdbot/moltbot.json â†’ models.providers.ollama

Moltbot æŠŠ Ollama è¦–ç‚º OpenAI-compatible provider

4.1 ç·¨è¼¯ container å…§çš„ moltbot.json
ç¯„ä¾‹ä½¿ç”¨æ¨¡åž‹æ˜¯:qwen2.5:7b-instruct-q4_K_M æ¨¡åž‹æœ‰æ›è«‹è‡ªå·±æ”¹


docker exec -it moltbot-moltbot-gateway-1 /bin/sh

cat > /home/node/.clawdbot/moltbot.json << 'EOF'
{
  "agents": {
    "defaults": {
      "model": {
        "primary": "ollama/qwen2.5:7b-instruct-q4_K_M"
      }
    }
  },
  "models": {
    "mode": "merge",
    "providers": {
      "ollama": {
        "baseUrl": "http://host.docker.internal:11434/v1",
        "apiKey": "ollama-local",
        "api": "openai-responses",
        "models": [
          {
            "id": "qwen2.5:7b-instruct-q4_K_M",
            "name": "Qwen 2.5 7B Instruct",
            "reasoning": false,
            "input": ["text"],
            "contextWindow": 16384, 
            "maxTokens": 512
          }
        ]
      }
    }
  }
}
EOF




é›¢é–‹ containerï¼š

exit

5ï¸âƒ£ é‡å•Ÿ Gateway è®“è¨­å®šç”Ÿæ•ˆ
docker compose restart moltbot-gateway

6ï¸âƒ£ ç¢ºèª Agent å·²å»ºç«‹
docker compose run --rm moltbot-cli agents list


é æœŸçœ‹åˆ°ï¼š

Agents:
- main (default)
  Model: ollama/qwen2.5:7b-instruct-q4_K_M

7ï¸âƒ£ å¯¦éš›æ¸¬è©¦ï¼ˆAgent æœ¬åœ°æŽ¨ç†ï¼‰
docker compose run --rm moltbot-cli agent \
  --local \
  --agent main \
  --message "ä½ å¥½ï¼Œè«‹ç”¨ä¸€å¥è©±ä»‹ç´¹ä½ è‡ªå·±"

æˆåŠŸåˆ¤æ–·æ–¹å¼

CLI å‡ºç¾ä¸­æ–‡å›žç­”

åŒæ™‚åœ¨ä¸»æ©ŸåŸ·è¡Œï¼š

ollama ps


å¯çœ‹åˆ°ï¼š

qwen2.5:7b-instruct-q4_K_M   running   100% GPU


ðŸ‘‰ ä»£è¡¨ Moltbot å·²é€éŽ Ollama ä½¿ç”¨ GPU æŽ¨ç†
8ï¸âƒ£ å¸¸è¦‹ç¾è±¡èˆ‡èªªæ˜Ž
ðŸ”¥ GPU 100%

ç¬¬ä¸€æ¬¡æŽ¨ç†æœƒï¼š

è¼‰å…¥æ¨¡åž‹

å»ºç«‹ KV cache
å±¬æ­£å¸¸ç¾è±¡ï¼Œå¾ŒçºŒæœƒå¿«å¾ˆå¤š
âš ï¸ low context window warning
warn<32000


åªæ˜¯æé†’ï¼Œä¸å½±éŸ¿åŸ·è¡Œ
Agent æ¨¡å¼æœ€ä½Žéœ€æ±‚ç‚º 16k context
9ï¸âƒ£ é—œéµè¸©é›·ç´€éŒ„ï¼ˆè¡€æ·šï¼‰

âŒ åœ¨ auth-profiles.json è¨­ Ollamaï¼ˆç„¡æ•ˆï¼‰

âŒ æœªæŒ‡å®š agents.defaults.model.primary

âŒ contextWindow < 16000ï¼ˆagent ç›´æŽ¥æ‹’çµ•ï¼‰

âŒ æ··ç”¨ anthropic / synthetic provider

âœ… æœ€çµ‚æˆæžœ

Docker åŒ– Moltbot Gateway + CLI

ä½¿ç”¨æœ¬åœ° Ollamaï¼ˆGPUï¼‰

Agent pipeline æ­£å¸¸é‹ä½œ

å®Œå…¨é›¢ç·šå¯ç”¨ï¼ˆé™¤ Docker æœ¬èº«ï¼‰
